# (generated with --quick)

import functools
import solutions.lesson1
import solutions.lesson2
from typing import Any, Callable, List, Sequence, Tuple, Type, TypeVar

Adam: Any
BasicCallback: Type[solutions.lesson2.BasicCallback]
Callback: Type[solutions.lesson2.Callback]
CombinedScheduler: Type[solutions.lesson2.CombinedScheduler]
CosineScheduler: Type[solutions.lesson2.CosineScheduler]
DataBunch: Type[solutions.lesson2.DataBunch]
DataLoader: Type[solutions.lesson2.DataLoader]
Dataset: Type[solutions.lesson2.Dataset]
F: Any
Learn: Type[solutions.lesson2.Learn]
Lin: Type[solutions.lesson1.Lin]
LinearScheduler: Type[solutions.lesson2.LinearScheduler]
LogSoftmax: Type[solutions.lesson2.LogSoftmax]
Loss: Type[solutions.lesson1.Loss]
MNIST_URL: str
MSE: Type[solutions.lesson1.MSE]
Model: Type[solutions.lesson2.Model]
ModelMode: Type[solutions.lesson2.ModelMode]
Module: Type[solutions.lesson1.Module]
Optimizer: Type[solutions.lesson2.Optimizer]
ReLU: Type[solutions.lesson1.ReLU]
Recorder: Type[solutions.lesson2.Recorder]
Runner: Type[solutions.lesson2.Runner]
Scheduler: Type[solutions.lesson2.Scheduler]
SequentialSampler: Type[solutions.lesson2.SequentialSampler]
X_train: Any
X_valid: Any
__all__: List[str]
download_data: Any
gzip: module
math: module
nn: module
np: module
partial: Type[functools.partial]
path: Any
pickle: module
tensor: module
torch: module
y_train: Any
y_valid: Any

_THooks = TypeVar('_THooks', bound=Hooks)

class AverageStatsCallback(solutions.lesson2.Callback):
    count: Any
    name: Any
    value: Any
    def __init__(self, f, name) -> None: ...
    def f(self, _1, _2) -> Any: ...
    def on_batch_end(self) -> None: ...
    def on_epoch_end(self) -> None: ...
    def on_epoch_start(self) -> None: ...

class BatchNorm(Any):
    beta: Any
    eps: Any
    gamma: Any
    mom: Any
    def __init__(self, nf, eps = ..., mom = ...) -> None: ...
    def forward(self, x) -> Any: ...
    def update_stats(self, x) -> Tuple[Any, Any]: ...

class GeneralReLU(Any):
    a: Any
    subtract: Any
    def __init__(self, a = ..., subtract = ...) -> None: ...
    def forward(self, x) -> Any: ...

class Hook:
    f: Any
    hook: Any
    m: Any
    def __del__(self) -> None: ...
    def __init__(self, f, m) -> None: ...
    def __repr__(self) -> str: ...
    def remove(self) -> None: ...

class Hooks(ListContainer):
    items: list
    def __del__(self) -> None: ...
    def __enter__(self: _THooks) -> _THooks: ...
    def __exit__(self, exc_type, exc_value, exc_traceback) -> None: ...
    def __init__(self, f, model) -> None: ...
    def remove(self) -> None: ...

class Lambda(Any):
    f: Any
    def __init__(self, f) -> None: ...
    def forward(self, x) -> Any: ...

class ListContainer:
    items: list
    def __getitem__(self, idx) -> Any: ...
    def __init__(self, *items) -> None: ...
    def __iter__(self) -> listiterator: ...
    def __repr__(self) -> str: ...
    def __setitem__(self, idx, item) -> None: ...
    def __str__(self) -> Any: ...

class RunningBatchNorm(Any):
    beta: Any
    eps: Any
    examples: Any
    gamma: Any
    mean: Any
    mom: Any
    var: Any
    def __init__(self, nf, eps = ..., mom = ...) -> None: ...
    def forward(self, x) -> Any: ...
    def update_stats(self, x) -> None: ...

def __getattr__(name) -> Any: ...
def accuracy(preds, actuals) -> Any: ...
def basic_model(nh, torch_softmax = ...) -> Any: ...
def conv_layer(ni, nf, size, stride = ..., bn = ..., **kwargs) -> Any: ...
def current_gpu() -> None: ...
def d_linear(inp, output, w, b) -> None: ...
def d_mse(output, y) -> None: ...
def d_relu(inp, output) -> None: ...
def fit(model, optimizer, dl, epochs = ...) -> None: ...
def forward_and_backward(x, y) -> Any: ...
def get_data(data_path) -> Tuple[nothing, nothing, nothing, nothing]: ...
def get_mnist_data() -> Tuple[nothing, nothing, nothing, nothing]: ...
def get_model() -> Any: ...
def get_runner(lr, loss, data, cbs) -> solutions.lesson2.Runner: ...
def init_cnn_(mdl) -> None: ...
def lin_scheduler(start: float, stop: float, hp: str) -> functools.partial: ...
def linear(w, x, b) -> Any: ...
def log_softmax(vec) -> Any: ...
def logsumexp(vec) -> Any: ...
def mse(y, y_hat) -> Any: ...
def nograd(f) -> Callable: ...
def relu(z) -> Any: ...
def stats(x) -> Tuple[Any, Any]: ...
def test_allclose(a, b) -> None: ...
def wraps(wrapped: Callable, assigned: Sequence[str] = ..., updated: Sequence[str] = ...) -> Callable[[Callable], Callable]: ...
