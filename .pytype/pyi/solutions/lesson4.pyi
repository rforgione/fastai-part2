# (generated with --quick)

import functools
import solutions.lesson1
import solutions.lesson2
import solutions.lesson3
from typing import Any, Callable, Generic, List, Sequence, Tuple, Type, TypeVar

Adam: Any
AverageStatsCallback: Type[solutions.lesson3.AverageStatsCallback]
BasicCallback: Type[solutions.lesson2.BasicCallback]
BatchNorm: Type[solutions.lesson3.BatchNorm]
Callback: Type[solutions.lesson2.Callback]
CombinedScheduler: Type[solutions.lesson2.CombinedScheduler]
CosineScheduler: Type[solutions.lesson2.CosineScheduler]
DataBunch: Type[solutions.lesson2.DataBunch]
DataLoader: Type[solutions.lesson2.DataLoader]
Dataset: Type[solutions.lesson2.Dataset]
F: Any
GeneralReLU: Type[solutions.lesson3.GeneralReLU]
Hook: Type[solutions.lesson3.Hook]
Hooks: Type[solutions.lesson3.Hooks]
Lambda: Type[solutions.lesson3.Lambda]
Learn: Type[solutions.lesson2.Learn]
Lin: Type[solutions.lesson1.Lin]
LinearScheduler: Type[solutions.lesson2.LinearScheduler]
ListContainer: Type[solutions.lesson3.ListContainer]
LogSoftmax: Type[solutions.lesson2.LogSoftmax]
Loss: Type[solutions.lesson1.Loss]
MNIST_URL: str
MSE: Type[solutions.lesson1.MSE]
Model: Type[solutions.lesson2.Model]
ModelMode: Type[solutions.lesson2.ModelMode]
Module: Type[solutions.lesson1.Module]
Optimizer: Type[solutions.lesson2.Optimizer]
ReLU: Type[solutions.lesson1.ReLU]
Recorder: Type[solutions.lesson2.Recorder]
Runner: Type[solutions.lesson2.Runner]
RunningBatchNorm: Type[solutions.lesson3.RunningBatchNorm]
Scheduler: Type[solutions.lesson2.Scheduler]
SequentialSampler: Type[solutions.lesson2.SequentialSampler]
X_train: Any
X_valid: Any
__all__: List[str]
download_data: Any
gzip: module
il: ItemList[int]
math: module
nn: module
np: module
partial: Type[functools.partial]
path: Any
pickle: module
tensor: module
torch: module
y_train: Any
y_valid: Any

A = TypeVar('A')
B = TypeVar('B')

class ItemList(solutions.lesson3.ListContainer, Generic[A]):
    items: List[int]
    def __init__(self, items: List[A]) -> None: ...
    def _get(self, idx, tfms: Sequence[Transform]) -> A: ...
    def get(self, idx: int) -> A: ...

class Transform(Generic[A, B]):
    _order: int
    def __call__(self: Transform[Any, nothing], x: A) -> B: ...

def __getattr__(name) -> Any: ...
def accuracy(preds, actuals) -> Any: ...
def basic_model(nh, torch_softmax = ...) -> Any: ...
def compose(x: A, fs: List[Callable[[A], A]]) -> A: ...
def conv_layer(ni, nf, size, stride = ..., bn = ..., **kwargs) -> Any: ...
def current_gpu() -> None: ...
def d_linear(inp, output, w, b) -> None: ...
def d_mse(output, y) -> None: ...
def d_relu(inp, output) -> None: ...
def f1(x) -> Any: ...
def f2(x) -> Any: ...
def fit(model, optimizer, dl, epochs = ...) -> None: ...
def forward_and_backward(x, y) -> Any: ...
def get_data(data_path) -> Tuple[nothing, nothing, nothing, nothing]: ...
def get_mnist_data() -> Tuple[nothing, nothing, nothing, nothing]: ...
def get_model() -> Any: ...
def get_runner(lr, loss, data, cbs) -> solutions.lesson2.Runner: ...
def init_cnn_(mdl) -> None: ...
def lin_scheduler(start: float, stop: float, hp: str) -> functools.partial: ...
def linear(w, x, b) -> Any: ...
def log_softmax(vec) -> Any: ...
def logsumexp(vec) -> Any: ...
def mse(y, y_hat) -> Any: ...
def nograd(f) -> Callable: ...
def relu(z) -> Any: ...
def stats(x) -> Tuple[Any, Any]: ...
def test_allclose(a, b) -> None: ...
def wraps(wrapped: Callable, assigned: Sequence[str] = ..., updated: Sequence[str] = ...) -> Callable[[Callable], Callable]: ...
