# (generated with --quick)

import functools
import solutions.lesson1
from typing import Any, Callable, Dict, Generator, List, Optional, Sequence, Tuple, Type, TypeVar

Lin: Type[solutions.lesson1.Lin]
Loss: Type[solutions.lesson1.Loss]
MNIST_URL: str
MSE: Type[solutions.lesson1.MSE]
Module: Type[solutions.lesson1.Module]
ReLU: Type[solutions.lesson1.ReLU]
X_train: Any
X_valid: Any
__all__: List[str]
gzip: module
math: module
nn: module
np: module
partial: Type[functools.partial]
path: Any
pickle: module
tensor: module
torch: module
y_train: Any
y_valid: Any

_TCallback = TypeVar('_TCallback', bound=Callback)
_TCombinedScheduler = TypeVar('_TCombinedScheduler', bound=CombinedScheduler)

class BasicCallback:
    _order: int
    runner: Runner
    def __init__(self, runner: Runner) -> None: ...
    def on_batch_end(self) -> Optional[bool]: ...

class Callback:
    _order: int
    runner: Runner
    def __call__(self: _TCallback, runner: Runner) -> _TCallback: ...
    def __getattr__(self, attr) -> None: ...
    def on_backward_end(self) -> None: ...
    def on_backward_start(self) -> None: ...
    def on_batch_end(self) -> None: ...
    def on_batch_start(self) -> None: ...
    def on_epoch_end(self) -> None: ...
    def on_epoch_start(self) -> None: ...
    def on_forward_end(self) -> None: ...
    def on_forward_start(self) -> None: ...
    def on_train_end(self) -> None: ...
    def on_train_start(self) -> None: ...

class CombinedScheduler(Scheduler):
    _order: int
    intervals: Any
    percentages: List[float]
    phases: Any
    runner: Runner
    scheduler_fns: Tuple[Callable[[Runner], Scheduler], ...]
    schedulers: Any
    def __call__(self: _TCombinedScheduler, runner: Runner) -> _TCombinedScheduler: ...
    def __init__(self, percentages: List[float], *scheduler_fns: Callable[[Runner], Scheduler]) -> None: ...
    def _build_phases(self, pcts: List[float]) -> List[Tuple[float, float]]: ...
    def _get_phase(self, pos: float) -> int: ...
    def build_schedulers(self, runner: Runner) -> None: ...
    def on_batch_end(self) -> None: ...

class CosineScheduler(Scheduler):
    _order: int
    hp: str
    start: float
    stop: float
    def __init__(self, start: float, stop: float, hp: str) -> None: ...
    def on_batch_end(self, pos: float = ...) -> None: ...

class DataBunch:
    c: Any
    test_dl: Any
    train_dl: Any
    valid_dl: Any
    def __init__(self, train_dl, valid_dl, test_dl = ..., c = ...) -> None: ...

class DataLoader:
    bs: Any
    ds: Any
    sampler: Any
    def __init__(self, ds, bs, sampler = ...) -> None: ...
    def __iter__(self) -> Generator[Any, Any, None]: ...
    def __len__(self) -> int: ...

class Dataset:
    x: Any
    y: Any
    def __getitem__(self, index) -> Tuple[Any, Any]: ...
    def __init__(self, x, y) -> None: ...
    def __len__(self) -> Any: ...

class Learn:
    data: Any
    loss: Any
    model: Any
    optimizer: Any
    def __init__(self, model, optimizer, data, loss) -> None: ...

class LinearScheduler(Scheduler):
    _order: int
    hp: str
    start: float
    stop: float
    def __init__(self, start: float, stop: float, hp: str) -> None: ...
    def on_batch_end(self, pos: float = ...) -> None: ...

class LogSoftmax(Any):
    input: Any
    output: Any
    def backward(self) -> None: ...
    def forward(self, x) -> Any: ...

class Model(Any):
    _modules: Dict[str, Any]
    def __call__(self, x) -> Any: ...
    def __init__(self, layers) -> None: ...

class ModelMode:
    TEST: int
    TRAIN: int
    VALID: int

class Optimizer:
    hp: Dict[str, Any]
    params: list
    step: Callable
    zero_grad: Callable
    def __init__(self, model, lr) -> None: ...

class Recorder(Callback):
    _order: int
    values: list
    def __init__(self) -> None: ...
    def on_batch_end(self) -> None: ...

class Runner:
    batches: int
    cbs: Any
    epochs: int
    learn: Any
    mode: Any
    pred: Any
    total_batches: Any
    train_counts: List[int]
    train_losses: list
    valid_counts: List[int]
    valid_losses: list
    xb: Any
    yb: Any
    def __call__(self, name) -> bool: ...
    def __init__(self, learn, cbs = ..., cb_funcs = ...) -> None: ...
    def all_batches(self) -> None: ...
    def fit(self, epochs = ...) -> None: ...
    def loss(self, mode: int) -> Any: ...
    def one_batch(self, xb, yb) -> Optional[bool]: ...
    def train(self) -> None: ...
    def valid(self) -> None: ...

class Scheduler(Callback): ...

class SequentialSampler:
    ds: Any
    def __init__(self, ds) -> None: ...
    def __iter__(self) -> Generator[int, Any, None]: ...
    def __len__(self) -> int: ...

def __getattr__(name) -> Any: ...
def basic_model(nh, torch_softmax = ...) -> Any: ...
def d_linear(inp, output, w, b) -> None: ...
def d_mse(output, y) -> None: ...
def d_relu(inp, output) -> None: ...
def fit(model, optimizer, dl, epochs = ...) -> None: ...
def forward_and_backward(x, y) -> Any: ...
def get_data(data_path) -> Tuple[nothing, nothing, nothing, nothing]: ...
def lin_scheduler(start: float, stop: float, hp: str) -> functools.partial: ...
def linear(w, x, b) -> Any: ...
def log_softmax(vec) -> Any: ...
def logsumexp(vec) -> Any: ...
def mse(y, y_hat) -> Any: ...
def nograd(f) -> Callable: ...
def relu(z) -> Any: ...
def test_allclose(a, b) -> None: ...
def wraps(wrapped: Callable, assigned: Sequence[str] = ..., updated: Sequence[str] = ...) -> Callable[[Callable], Callable]: ...
