{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lesson2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/development/fastai-part2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo = git.Repo(search_parent_directories=True)\n",
    "repo_root = repo.git.rev_parse(\"--show-toplevel\")\n",
    "os.chdir(repo_root)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from solutions.lesson1 import *\n",
    "from fastai.datasets import *\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from functools import wraps\n",
    "from typing import List, Tuple, Callable\n",
    "from functools import partial\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/development/fastai-part2/solutions/lesson1.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
    "path = download_data(MNIST_URL, ext=\".gz\")\n",
    "X_train, y_train, X_valid, y_valid = get_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([10000, 784]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000]), torch.Size([10000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        self._modules = {}\n",
    "        for idx, layer in enumerate(layers):\n",
    "            self._modules[f'layer{idx}'] = layer\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for _, layer in self.named_children(): x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50\n",
    "ni = X_train.shape[1]\n",
    "c = int(y_train.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([\n",
    "    nn.Linear(ni, nh),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nh, c)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer1): ReLU()\n",
       "  (layer2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 10]),\n",
       " tensor([[ 0.1649, -0.1323, -0.1332,  ...,  0.0178,  0.1816, -0.1312],\n",
       "         [ 0.2021, -0.1218, -0.1044,  ...,  0.0237,  0.1407, -0.1884],\n",
       "         [ 0.1784, -0.1351,  0.0800,  ..., -0.0435,  0.0637, -0.2104],\n",
       "         ...,\n",
       "         [ 0.2268, -0.1062, -0.0394,  ...,  0.0007,  0.0349, -0.1669],\n",
       "         [ 0.2270, -0.1668, -0.0366,  ...,  0.0573,  0.1204, -0.1750],\n",
       "         [ 0.2221, -0.1500, -0.0601,  ..., -0.0797,  0.0627, -0.1602]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the naive way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return (vec.exp() / vec.exp().sum(-1)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert log_softmax(torch.tensor([[1.,2.,3.]])).exp().sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the numerically friendly way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return vec - vec.exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert log_softmax(torch.tensor([[1.,2.,3.]])).exp().sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(output, target):\n",
    "    return -output[range(len(output)), target].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_output = torch.tensor([\n",
    "    [0.5, 0.5],\n",
    "    [0.3, 0.7]\n",
    "])\n",
    "\n",
    "fake_target = torch.tensor([\n",
    "    0,\n",
    "    1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_first = np.log(0.5); log_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.35667494393873245"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_second = np.log(0.7); log_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_nll = -(log_first + log_second)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5249)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(torch.log(fake_output), fake_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(nll(fake_output.log(), fake_target), torch.tensor(correct_nll).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogSumExp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogSumExp is a way of making the sum of exponents more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(vec):\n",
    "    max_elem = vec.max()\n",
    "    return max_elem + (vec - max_elem).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.tensor([1,2,3]).float().exp().sum().log() == logsumexp(torch.tensor([1,2,3]).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way of making the log of a sum of exponentials more numerically stable. You can take advantage of the fact that dividing two exponentials results in subtracting their exponents. You can factor out the largest exponential, which subtracts its exponent from each of the other exponents. Then you can take advantage of the log rules that state that log(ab) = log(a) + log(b), which allows you to add the highest exponential to the log of the sum of the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logsumexp(torch.tensor([1,2,3]).float()) == torch.logsumexp(torch.tensor([1,2,3]).float(), dim=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_softmax in terms of LogSumExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return vec - vec.logsumexp(dim=(0,), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(torch.tensor([1,2,3]).float()) == torch.log_softmax(torch.tensor([1,2,3]).float(), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 5],\n",
       "       [5, 6, 7]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2,3], [3,4,5], [5,6,7]])[:, [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_example = torch.tensor([\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_example[range(len(grad_example)), grad_example.argmax(dim=1)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 2],\n",
       "        [2, 0, 0],\n",
       "        [0, 2, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [1, 0],\n",
       "        [2, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 2],\n",
       "         [0, 2, 0]],\n",
       "\n",
       "        [[2, 0, 0],\n",
       "         [0, 0, 2]],\n",
       "\n",
       "        [[0, 2, 0],\n",
       "         [2, 0, 0]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example[grad_example.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0,0,0,1,2],\n",
    "    [3,5,3,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 5, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero = t.nonzero()\n",
    "xs = nonzero[:,0]\n",
    "ys = nonzero[:,1]\n",
    "t[xs, ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = torch.zeros_like(t)\n",
    "tz[xs, ys] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(nn.Module):\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = torch.log_softmax(self.input, dim=-1)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self): \n",
    "        correct_labels = (self.output.grad > 0).float()\n",
    "        msg = \"LogSoftmax must be used with CE loss, found more than one output dim with nonzero gradients for the same example\"\n",
    "        assert correct_labels.sum(axis=0).max() == 1, msg\n",
    "        self.input.grad = (correct_labels - self.output) * self.output.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([50000, 10]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = Model([\n",
    "    nn.Linear(784,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, c),\n",
    "    LogSoftmax()\n",
    "])\n",
    "X_train.shape, mdl(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2 = Model([\n",
    "    nn.Linear(784,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, c),\n",
    "    nn.LogSoftmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1192, 0.0925, 0.1062,  ..., 0.0985, 0.0800, 0.1085],\n",
       "        [0.1168, 0.0898, 0.1032,  ..., 0.0934, 0.0867, 0.1223],\n",
       "        [0.1158, 0.0884, 0.1044,  ..., 0.0942, 0.0923, 0.1078],\n",
       "        ...,\n",
       "        [0.1084, 0.0973, 0.1081,  ..., 0.0914, 0.0908, 0.1039],\n",
       "        [0.1211, 0.1035, 0.0968,  ..., 0.0861, 0.0931, 0.1101],\n",
       "        [0.1126, 0.0941, 0.0981,  ..., 0.0851, 0.0912, 0.1071]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(mdl(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss(mdl(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3]]).float()\n",
    "l = LogSoftmax()\n",
    "l.forward(a)\n",
    "l.output.grad = torch.tensor([[0,0,10]]).float()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4076, -1.4076, -0.4076]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log_softmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000, 14.0761]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(log_softmax(torch.tensor([1,2,3]).float()), torch.log_softmax(torch.tensor([1,2,3]).float(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer0', Linear(in_features=784, out_features=100, bias=True)), ('layer1', ReLU()), ('layer2', Linear(in_features=100, out_features=10, bias=True)), ('layer3', LogSoftmax())]\n"
     ]
    }
   ],
   "source": [
    "print(list(mdl2.named_children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, bs=64, lr=1e-3, epochs=1):\n",
    "    n_batches = X_train.shape[0] // bs + 1\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * bs\n",
    "            end_idx = (i + 1) * bs\n",
    "            X_b, y_b = X_train[start_idx:end_idx], y_train[start_idx:end_idx] \n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _, layer in model.named_children():\n",
    "                    if hasattr(layer, 'weight'):\n",
    "                        layer.weight.sub_(layer.weight.grad * lr)\n",
    "                        layer.bias.sub_(layer.bias.grad * lr)\n",
    "                        layer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (layer1): ReLU()\n",
       "  (layer2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (layer3): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has it\n",
      "has it\n"
     ]
    }
   ],
   "source": [
    "for i,l in mdl.named_children(): \n",
    "    if hasattr(l, 'weight'): \n",
    "        print('has it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(mdl2, 64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(mdl, 64, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nograd(f):\n",
    "    @wraps(f)\n",
    "    def inner(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return f(*args, **kwargs)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.hp = {\n",
    "            \"lr\": lr\n",
    "        }\n",
    "        params = []\n",
    "        for _, layer in model.named_children():\n",
    "            if hasattr(layer, 'weight'):\n",
    "                params.append(layer)\n",
    "        self.params = params\n",
    "    \n",
    "    @nograd\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.weight.sub_(param.weight.grad * self.hp['lr'])\n",
    "            param.bias.sub_(param.bias.grad * self.hp['lr'])\n",
    "                \n",
    "    @nograd\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.zero_grad() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, bs=64, epochs=1):\n",
    "    n_batches = X_train.shape[0] // bs\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * bs\n",
    "            end_idx = (i + 1) * bs\n",
    "            X_b, y_b = X_train[start_idx:end_idx], y_train[start_idx:end_idx] \n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def basic_model(nh, torch_softmax=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(784, nh),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(nh, 10),\n",
    "        nn.LogSoftmax() if torch_softmax else LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = basic_model(100)\n",
    "opt = Optimizer(mdl, 1e-3)\n",
    "#fit(mdl, opt, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2 = basic_model(100, True)\n",
    "opt2 = Optimizer(mdl2, 1e-3)\n",
    "#fit(mdl2, opt2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Dataset:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([4, 1, 9]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [(1,2), (3,4), (5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 5), (2, 4, 6)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SequentialSampler:\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.ds)):\n",
    "            yield(i)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataLoader:\n",
    "    def __init__(self, ds, bs, sampler=None):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "        \n",
    "        if callable(sampler):\n",
    "            self.sampler = sampler(ds)\n",
    "        else:\n",
    "            self.sampler = sampler or SequentialSampler(ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        it = iter(self.sampler)\n",
    "        nbatches = math.ceil(len(self) / self.bs)\n",
    "        for _ in range(nbatches):\n",
    "            try:\n",
    "                idxs = []\n",
    "                for _ in range(self.bs):\n",
    "                    idxs.append(next(it))\n",
    "                yield self.ds[idxs]\n",
    "            except StopIteration:\n",
    "                yield self.ds[idxs] # flush the final partial batch\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(d, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(yb) for xb,yb in dl]), max([len(yb) for xb,yb in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, dl, epochs=1):\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for X_b, y_b in dl:\n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = basic_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(mod, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(mod, opt, dl, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    _order = 0\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_batch_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_forward_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_forward_end(self): \n",
    "        pass\n",
    "    \n",
    "    def on_backward_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_backward_end(self): \n",
    "        pass\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Learn:\n",
    "    def __init__(self, model, optimizer, data, loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.data = data\n",
    "        self.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CallbackHandler:\n",
    "#     def __init__(self, learn, callbacks, callback_fns):\n",
    "#         self.callbacks = [cb(learn) for cb in callback_fns] + callbacks \n",
    "        \n",
    "#     def on_train_start(self):\n",
    "#         for callback in self.callbacks: callback.on_train_start()\n",
    "        \n",
    "#     def on_epoch_start(self):\n",
    "#         for callback in self.callbacks: callback.on_epoch_start()\n",
    "        \n",
    "#     def on_batch_start(self): \n",
    "#         for callback in self.callbacks: callback.on_batch_start()\n",
    "            \n",
    "#     def on_forward_start(self): \n",
    "#         for callback in self.callbacks: callback.on_forward_start()\n",
    "            \n",
    "#     def on_forward_end(self): \n",
    "#         for callback in self.callbacks: callback.on_forward_end()\n",
    "            \n",
    "#     def on_backward_start(self): \n",
    "#         for callback in self.callbacks: callback.on_backward_start()\n",
    "            \n",
    "#     def on_backward_end(self): \n",
    "#         for callback in self.callbacks: callback.on_backward_end()\n",
    "        \n",
    "#     def on_batch_end(self):\n",
    "#         for callback in self.callbacks: callback.on_batch_end()\n",
    "        \n",
    "#     def on_epoch_end(self):\n",
    "#         for callback in self.callbacks: callback.on_epoch_end()\n",
    "        \n",
    "#     def on_train_end(self):\n",
    "#         for callback in self.callbacks: callback.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CallbackHandler(Callback):\n",
    "#     def __init__(self, learn, callbacks, callback_fns):\n",
    "#         self.callbacks = [cb(learn) for cb in callback_fns] + callbacks \n",
    "        \n",
    "#     def __getattr__(self, attr):\n",
    "#         if attr.startswith('on_'):\n",
    "#             for callback in sorted(self.callbacks, key=lambda x: x._order): \n",
    "#                 f = getattr(callback, attr) \n",
    "#                 if f and f(): return False # false means don't stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ModelMode:\n",
    "    TRAIN = 0\n",
    "    VALID = 1\n",
    "    TEST = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBunch:\n",
    "    def __init__(self, train_dl, valid_dl, test_dl=None, c=None):\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Runner(object): \n",
    "    def __init__(self, learn, cbs=None, cb_funcs=None):\n",
    "        self.learn = learn\n",
    "        cb_funcs = cb_funcs or []\n",
    "        cbs = cbs or []\n",
    "        self.cbs = cbs + [cb_func(self) for cb_func in cb_funcs]\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.train_counts = []\n",
    "        self.valid_losses = []\n",
    "        self.valid_counts = []\n",
    "        \n",
    "        self.batches = 0\n",
    "        \n",
    "    def train(self):\n",
    "        self.mode = ModelMode.TRAIN\n",
    "        \n",
    "    def valid(self):\n",
    "        self.mode = ModelMode.VALID\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.mode \n",
    "        \n",
    "    def one_batch(self, xb, yb):\n",
    "        self.xb, self.yb = xb, yb\n",
    "        pred = self.learn.model(xb)\n",
    "        loss = self.learn.loss(pred, yb) \n",
    "         \n",
    "        if self.mode == ModelMode.TRAIN:\n",
    "            if self('on_batch_start'): return\n",
    "            self.batches += 1\n",
    "            self.train_losses.append(loss.item())\n",
    "            self.train_counts.append(len(xb))\n",
    "            loss.backward()\n",
    "            if not self('on_step_start'): self.learn.optimizer.step()\n",
    "            self.learn.optimizer.zero_grad()\n",
    "            self('on_step_end') \n",
    "            if self('on_batch_end'): return True\n",
    "            \n",
    "        if self.mode == ModelMode.VALID:\n",
    "            self.valid_losses.append(loss.item())     \n",
    "            self.valid_counts.append(len(xb))\n",
    "        \n",
    "    def all_batches(self):\n",
    "        if self('on_epoch_start'): return\n",
    "        self.epochs += 1\n",
    "        dl = self.learn.data.train_dl if self.mode == ModelMode.TRAIN else self.learn.data.valid_dl\n",
    "        for xb, yb in dl:\n",
    "            self.one_batch(xb, yb)\n",
    "        self('on_epoch_end') \n",
    "        \n",
    "    def fit(self, epochs=1):\n",
    "        self.total_batches = math.ceil(len(learn.data.train_dl) / learn.data.train_dl.bs) * epochs\n",
    "        self('on_train_start')\n",
    "        self.epochs = 0\n",
    "        for _ in range(epochs):\n",
    "            self.train()     \n",
    "            if self.all_batches(): return\n",
    "            \n",
    "            self.valid()\n",
    "            if self.all_batches(): return\n",
    "            print(\"Validation loss: {0:.4f}\".format(self.loss(ModelMode.VALID)))\n",
    "            self.epochs += 1\n",
    "        self('on_train_end')\n",
    "        \n",
    "    def loss(self, mode: int):\n",
    "        if mode == ModelMode.TRAIN:\n",
    "            values, counts = self.train_losses, self.train_counts\n",
    "        elif mode == ModelMode.VALID:\n",
    "            values, counts = self.valid_losses, self.valid_counts\n",
    "        else:\n",
    "            raise ValueError(\"mode must be one of ModelMode.TRAIN or ModelMode.VALID.\")\n",
    "            \n",
    "        total = sum(counts)\n",
    "        weights = [i/total for i in counts]\n",
    "        return sum([value*weight for value, weight in zip(values, weights)]) \n",
    "        \n",
    "    def __call__(self, name):\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, name, None)\n",
    "            if f and f(): return True\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4,  ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([3, 8, 6,  ..., 5, 6, 8]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(X_train, y_train)\n",
    "valid_ds = Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, 64)\n",
    "valid_dl = DataLoader(valid_ds, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "mdl = basic_model(100)\n",
    "opt = Optimizer(mdl, 1e-3)\n",
    "learn = Learn(mdl, opt, data, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_runner = Runner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_runner.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_runner.loss(ModelMode.VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BasicCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCallback:\n",
    "    _order = 0\n",
    "    \n",
    "    def __init__(self, runner: Runner):\n",
    "        self.runner = runner\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        print(self.runner.batches)\n",
    "        if self.runner.batches == 10:\n",
    "            print(\"Done!\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_runner_cb = Runner(learn, cb_funcs=[BasicCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_runner_cb.fit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler:\n",
    "    _order = 0\n",
    "    def __init__(self, start: float, stop: float, hp: str, runner: Runner):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.runner = runner \n",
    "        self.hp = hp\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        pos = self.runner.batches / self.runner.total_batches\n",
    "        new_hp = start + (self.stop - self.start) * pos\n",
    "        self.runner.learn.optimizer.hp[self.hp] = new_hp\n",
    "        if pos % 0.1 == 0:\n",
    "            print(\"The {} is now {}\".format(self.hp, new_hp)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_scheduler(start: float, stop: float, hp: str) -> partial: return partial(LinearScheduler, start, stop, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<class '__main__.LinearScheduler'>, 10, 50, 'lr')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_scheduler(10, 50, 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    pass\n",
    "\n",
    "class CosineScheduler(Scheduler):\n",
    "    _order = 0\n",
    "    def __init__(self, start: float, stop: float, hp: str, runner: Runner):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.runner = runner \n",
    "        self.hp = hp\n",
    "        \n",
    "    def on_batch_end(self, pos: float = None):\n",
    "        pos = pos or self.runner.batches / self.runner.total_batches\n",
    "        new_hp = self.start + (1 + math.cos(math.pi*(1-pos))) * (self.stop - self.start) / 2\n",
    "        self.runner.learn.optimizer.hp[self.hp] = new_hp\n",
    "        if pos % 0.1 == 0 or pos == 0 or pos == 1:\n",
    "            print(\"The {} is now {} for pos {} (batch {})\".format(self.hp, new_hp, pos, self.runner.batches)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(len(data.train_dl) / data.train_dl.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_scheduler(start: float, stop: float, hp: str) -> partial: return partial(CosineScheduler, start, stop, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_cbs = Runner(learn, cb_funcs=[cos_scheduler(1e-5, 1e-1, 'lr')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner_cbs.fit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to:\n",
    "1. Check to see what percentage of batches have been processed\n",
    "2. Depending on what percentage we're up to, determine which scheduler to use\n",
    "3. For that scheduler, calculate the HP value based on what point in the phase we're at\n",
    "\n",
    "Each one treats percentages independently; so if the first phase needs to go from 0.3 to 0.8 over the first 70% of batches, and there are 100 batches total, then batch 70 needs to look like the \"100%\" mark for the first scheduler. What we can do is:\n",
    "* take these percentages and construct intervals, which are tuples of batch indices (e.g. (0,70) for the example above). \n",
    "* Select the interval for the batch that we're currently in\n",
    "* Subtract batch num - interval_beginning / (interval_end - interval_beginning) to get the percentage\n",
    "* Pass that percentage to the scheduler associated with that interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedScheduler(Scheduler):\n",
    "    _order = 0\n",
    "    def __init__(self, percentages: List[float], *scheduler_fns: Callable[[Runner], Scheduler]):\n",
    "        assert sum(percentages) == 1.\n",
    "        self.percentages = percentages\n",
    "        self.intervals = [(percentages[i-1], percentages[i]) for i in range(0,len(percentages))]\n",
    "        self.phases = self._build_phases(percentages)\n",
    "        self.scheduler_fns = scheduler_fns\n",
    "        self.schedulers = None\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        if self.schedulers is None:\n",
    "            print(\"Must build the schedulers by calling build_schedulers(runner) before calling on_batch_end.\")\n",
    "        runner = self.schedulers[0].runner\n",
    "        # all schedulers must refer to the same runner\n",
    "        assert all([i.runner == runner for i in self.schedulers])\n",
    "        pos = runner.batches / runner.total_batches\n",
    "        phase_idx = self._get_phase(pos)\n",
    "        phase = self.phases[phase_idx]\n",
    "        scheduler = self.schedulers[phase_idx]\n",
    "        phase_pct = pos - phase[0]\n",
    "        phase_size = phase[1] - phase[0]\n",
    "        effective_pct = phase_pct / phase_size \n",
    "        scheduler.on_batch_end(effective_pct)\n",
    "        \n",
    "    def _build_phases(self, pcts: List[float]) -> List[Tuple[float, float]]:\n",
    "        pcts = [0] + pcts\n",
    "        cumsum = torch.cumsum(torch.tensor(pcts), dim=-1).tolist()\n",
    "        intervals = [(round(cumsum[i-1], 1), round(cumsum[i], 1)) for i in range(1, len(cumsum))]\n",
    "        return intervals\n",
    "    \n",
    "    def _get_phase(self, pos: float) -> int:\n",
    "        for idx, phase in enumerate(self.phases):\n",
    "            if phase[0] < pos <= phase[1]:\n",
    "                return idx \n",
    "            \n",
    "    def build_schedulers(self, runner: Runner):\n",
    "        self.schedulers = [i(runner) for i in self.scheduler_fns]\n",
    "         \n",
    "    def __call__(self, runner: Runner):\n",
    "        self.build_schedulers(runner)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = [0.7, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3, 0.7), (0.7, 0.3)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(percentages[i-1], percentages[i]) for i in range(0,len(percentages))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7000, 1.0000])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(torch.tensor(percentages), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phases(pcts: List[float]) -> List[Tuple[float, float]]:\n",
    "    pcts = [0] + pcts\n",
    "    cumsum = torch.cumsum(torch.tensor(pcts), dim=-1).tolist()\n",
    "    intervals = [(round(cumsum[i-1], 1), round(cumsum[i], 1)) for i in range(1, len(cumsum))]\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.7), (0.7, 1.0)]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_phases([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert build_phases([0.7, 0.3]) == [(0, 0.7), (0.7, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase(pos: float, phases: List[Tuple[float, float]]) -> int:\n",
    "    for idx, phase in enumerate(phases):\n",
    "        if phase[0] < pos <= phase[1]:\n",
    "            return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_phase(0.5, [(0, 0.7), (0.7, 1.0)]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_phase(0.8, [(0, 0.7), (0.7, 1.0)]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder:\n",
    "    _order=1\n",
    "    \n",
    "    def __init__(self, runner: Runner):\n",
    "        self.runner = runner\n",
    "        self.values = []\n",
    "    \n",
    "    def on_batch_end(self):\n",
    "        self.values.append(self.runner.learn.optimizer.hp.copy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basic_model(10)\n",
    "opt = Optimizer(model, 1e-3)\n",
    "learn = Learn(model, opt, data, nn.NLLLoss())\n",
    "scheduler = CombinedScheduler([0.3, 0.7], cos_scheduler(1e-5, 1e-1, 'lr'), cos_scheduler(1e-1, 1e-3, 'lr'))\n",
    "run = Runner(learn, cb_funcs=[scheduler, Recorder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.9420\n",
      "Validation loss: 0.6580\n",
      "The lr is now 0.1 for pos 1.0 (batch 2346)\n",
      "Validation loss: 0.5418\n",
      "Validation loss: 0.4766\n",
      "Validation loss: 0.4344\n",
      "Validation loss: 0.4046\n",
      "Validation loss: 0.3818\n",
      "Validation loss: 0.3633\n",
      "Validation loss: 0.3484\n",
      "The lr is now 0.0010000000000000009 for pos 1.0 (batch 7820)\n",
      "Validation loss: 0.3365\n"
     ]
    }
   ],
   "source": [
    "run.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [i['lr'] for i in run.cbs[1].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9aa8b26e90>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(lrs).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
