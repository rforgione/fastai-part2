{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lesson2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/development/fastai-part2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo = git.Repo(search_parent_directories=True)\n",
    "repo_root = repo.git.rev_parse(\"--show-toplevel\")\n",
    "os.chdir(repo_root)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from solutions.lesson1 import *\n",
    "from fastai.datasets import *\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from functools import wraps\n",
    "from typing import List\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/development/fastai-part2/solutions/lesson1.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
    "path = download_data(MNIST_URL, ext=\".gz\")\n",
    "X_train, y_train, X_valid, y_valid = get_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([10000, 784]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000]), torch.Size([10000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        self._modules = {}\n",
    "        for idx, layer in enumerate(layers):\n",
    "            self._modules[f'layer{idx}'] = layer\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for _, layer in self.named_children(): x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50\n",
    "ni = X_train.shape[1]\n",
    "c = int(y_train.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([\n",
    "    nn.Linear(ni, nh),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nh, c)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer1): ReLU()\n",
       "  (layer2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 10]),\n",
       " tensor([[-0.0227, -0.1500, -0.0351,  ..., -0.2415, -0.0135,  0.0365],\n",
       "         [-0.1056, -0.1039, -0.0290,  ..., -0.0922,  0.0605,  0.0636],\n",
       "         [-0.0506, -0.1094, -0.1007,  ..., -0.1133, -0.0943, -0.0144],\n",
       "         ...,\n",
       "         [-0.0099, -0.0921,  0.0195,  ..., -0.1998, -0.0890,  0.0466],\n",
       "         [-0.0792, -0.0760, -0.0530,  ..., -0.1133, -0.1084, -0.0250],\n",
       "         [-0.0773, -0.0868, -0.0572,  ..., -0.1725, -0.1122,  0.0985]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the naive way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return (vec.exp() / vec.exp().sum(-1)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert log_softmax(torch.tensor([[1.,2.,3.]])).exp().sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the numerically friendly way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return vec - vec.exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert log_softmax(torch.tensor([[1.,2.,3.]])).exp().sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(output, target):\n",
    "    return -output[range(len(output)), target].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_output = torch.tensor([\n",
    "    [0.5, 0.5],\n",
    "    [0.3, 0.7]\n",
    "])\n",
    "\n",
    "fake_target = torch.tensor([\n",
    "    0,\n",
    "    1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_first = np.log(0.5); log_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.35667494393873245"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_second = np.log(0.7); log_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_nll = -(log_first + log_second)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5249)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(torch.log(fake_output), fake_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(nll(fake_output.log(), fake_target), torch.tensor(correct_nll).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogSumExp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogSumExp is a way of making the sum of exponents more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(vec):\n",
    "    max_elem = vec.max()\n",
    "    return max_elem + (vec - max_elem).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.tensor([1,2,3]).float().exp().sum().log() == logsumexp(torch.tensor([1,2,3]).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way of making the log of a sum of exponentials more numerically stable. You can take advantage of the fact that dividing two exponentials results in subtracting their exponents. You can factor out the largest exponential, which subtracts its exponent from each of the other exponents. Then you can take advantage of the log rules that state that log(ab) = log(a) + log(b), which allows you to add the highest exponential to the log of the sum of the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logsumexp(torch.tensor([1,2,3]).float()) == torch.logsumexp(torch.tensor([1,2,3]).float(), dim=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_softmax in terms of LogSumExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return vec - vec.logsumexp(dim=(0,), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(torch.tensor([1,2,3]).float()) == torch.log_softmax(torch.tensor([1,2,3]).float(), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 5],\n",
       "       [5, 6, 7]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2,3], [3,4,5], [5,6,7]])[:, [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_example = torch.tensor([\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_example[range(len(grad_example)), grad_example.argmax(dim=1)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 2],\n",
       "        [2, 0, 0],\n",
       "        [0, 2, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [1, 0],\n",
       "        [2, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 2],\n",
       "         [0, 2, 0]],\n",
       "\n",
       "        [[2, 0, 0],\n",
       "         [0, 0, 2]],\n",
       "\n",
       "        [[0, 2, 0],\n",
       "         [2, 0, 0]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example[grad_example.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0,0,0,1,2],\n",
    "    [3,5,3,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 5, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero = t.nonzero()\n",
    "xs = nonzero[:,0]\n",
    "ys = nonzero[:,1]\n",
    "t[xs, ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = torch.zeros_like(t)\n",
    "tz[xs, ys] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(nn.Module):\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = torch.log_softmax(self.input, dim=-1)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self): \n",
    "        correct_labels = (self.output.grad > 0).float()\n",
    "        msg = \"LogSoftmax must be used with CE loss, found more than one output dim with nonzero gradients for the same example\"\n",
    "        assert correct_labels.sum(axis=0).max() == 1, msg\n",
    "        self.input.grad = (correct_labels - self.output) * self.output.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([50000, 10]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = Model([\n",
    "    nn.Linear(784,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, c),\n",
    "    LogSoftmax()\n",
    "])\n",
    "X_train.shape, mdl(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2 = Model([\n",
    "    nn.Linear(784,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, c),\n",
    "    nn.LogSoftmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0943, 0.0986, 0.1012,  ..., 0.0999, 0.0854, 0.0861],\n",
       "        [0.0832, 0.0902, 0.1107,  ..., 0.0980, 0.0924, 0.0902],\n",
       "        [0.0928, 0.0926, 0.1067,  ..., 0.0994, 0.0893, 0.0830],\n",
       "        ...,\n",
       "        [0.0903, 0.0872, 0.1050,  ..., 0.0904, 0.0860, 0.0951],\n",
       "        [0.1003, 0.0916, 0.1001,  ..., 0.1039, 0.0812, 0.0893],\n",
       "        [0.1017, 0.1020, 0.1030,  ..., 0.0942, 0.0852, 0.0853]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(mdl(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss(mdl(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3]]).float()\n",
    "l = LogSoftmax()\n",
    "l.forward(a)\n",
    "l.output.grad = torch.tensor([[0,0,10]]).float()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4076, -1.4076, -0.4076]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log_softmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000, 14.0761]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(log_softmax(torch.tensor([1,2,3]).float()), torch.log_softmax(torch.tensor([1,2,3]).float(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer0', Linear(in_features=784, out_features=100, bias=True)), ('layer1', ReLU()), ('layer2', Linear(in_features=100, out_features=10, bias=True)), ('layer3', LogSoftmax())]\n"
     ]
    }
   ],
   "source": [
    "print(list(mdl2.named_children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, bs=64, lr=1e-3, epochs=1):\n",
    "    n_batches = X_train.shape[0] // bs + 1\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * bs\n",
    "            end_idx = (i + 1) * bs\n",
    "            X_b, y_b = X_train[start_idx:end_idx], y_train[start_idx:end_idx] \n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _, layer in model.named_children():\n",
    "                    if hasattr(layer, 'weight'):\n",
    "                        layer.weight.sub_(layer.weight.grad * lr)\n",
    "                        layer.bias.sub_(layer.bias.grad * lr)\n",
    "                        layer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (layer1): ReLU()\n",
       "  (layer2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (layer3): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has it\n",
      "has it\n"
     ]
    }
   ],
   "source": [
    "for i,l in mdl.named_children(): \n",
    "    if hasattr(l, 'weight'): \n",
    "        print('has it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.pyenv/versions/miniconda3-3.7.0/envs/fastai/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1683316230773926\n"
     ]
    }
   ],
   "source": [
    "fit(mdl2, 64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42841383814811707\n"
     ]
    }
   ],
   "source": [
    "fit(mdl, 64, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nograd(f):\n",
    "    @wraps(f)\n",
    "    def inner(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return f(*args, **kwargs)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.hp = {\n",
    "            \"lr\": lr\n",
    "        }\n",
    "        params = []\n",
    "        for _, layer in model.named_children():\n",
    "            if hasattr(layer, 'weight'):\n",
    "                params.append(layer)\n",
    "        self.params = params\n",
    "    \n",
    "    @nograd\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.weight.sub_(param.weight.grad * self.hp['lr'])\n",
    "            param.bias.sub_(param.bias.grad * self.hp['lr'])\n",
    "                \n",
    "    @nograd\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.zero_grad() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, bs=64, epochs=1):\n",
    "    n_batches = X_train.shape[0] // bs\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * bs\n",
    "            end_idx = (i + 1) * bs\n",
    "            X_b, y_b = X_train[start_idx:end_idx], y_train[start_idx:end_idx] \n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def basic_model(nh, torch_softmax=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(784, nh),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(nh, 10),\n",
    "        nn.LogSoftmax() if torch_softmax else LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = basic_model(100)\n",
    "opt = Optimizer(mdl, 1e-3)\n",
    "#fit(mdl, opt, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2 = basic_model(100, True)\n",
    "opt2 = Optimizer(mdl2, 1e-3)\n",
    "#fit(mdl2, opt2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Dataset:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([4, 1, 9]))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [(1,2), (3,4), (5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 5), (2, 4, 6)]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SequentialSampler:\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.ds)):\n",
    "            yield(i)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataLoader:\n",
    "    def __init__(self, ds, bs, sampler=None):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "        \n",
    "        if callable(sampler):\n",
    "            self.sampler = sampler(ds)\n",
    "        else:\n",
    "            self.sampler = sampler or SequentialSampler(ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        it = iter(self.sampler)\n",
    "        nbatches = math.ceil(len(self) / self.bs)\n",
    "        for _ in range(nbatches):\n",
    "            try:\n",
    "                idxs = []\n",
    "                for _ in range(self.bs):\n",
    "                    idxs.append(next(it))\n",
    "                yield self.ds[idxs]\n",
    "            except StopIteration:\n",
    "                yield self.ds[idxs] # flush the final partial batch\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(d, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 64)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(yb) for xb,yb in dl]), max([len(yb) for xb,yb in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, dl, epochs=1):\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for X_b, y_b in dl:\n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = basic_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(mod, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit(mod, opt, dl, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    _order = 0\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_batch_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_forward_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_forward_end(self): \n",
    "        pass\n",
    "    \n",
    "    def on_backward_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_backward_end(self): \n",
    "        pass\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Learn:\n",
    "    def __init__(self, model, optimizer, data, loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.data = data\n",
    "        self.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CallbackHandler:\n",
    "#     def __init__(self, learn, callbacks, callback_fns):\n",
    "#         self.callbacks = [cb(learn) for cb in callback_fns] + callbacks \n",
    "        \n",
    "#     def on_train_start(self):\n",
    "#         for callback in self.callbacks: callback.on_train_start()\n",
    "        \n",
    "#     def on_epoch_start(self):\n",
    "#         for callback in self.callbacks: callback.on_epoch_start()\n",
    "        \n",
    "#     def on_batch_start(self): \n",
    "#         for callback in self.callbacks: callback.on_batch_start()\n",
    "            \n",
    "#     def on_forward_start(self): \n",
    "#         for callback in self.callbacks: callback.on_forward_start()\n",
    "            \n",
    "#     def on_forward_end(self): \n",
    "#         for callback in self.callbacks: callback.on_forward_end()\n",
    "            \n",
    "#     def on_backward_start(self): \n",
    "#         for callback in self.callbacks: callback.on_backward_start()\n",
    "            \n",
    "#     def on_backward_end(self): \n",
    "#         for callback in self.callbacks: callback.on_backward_end()\n",
    "        \n",
    "#     def on_batch_end(self):\n",
    "#         for callback in self.callbacks: callback.on_batch_end()\n",
    "        \n",
    "#     def on_epoch_end(self):\n",
    "#         for callback in self.callbacks: callback.on_epoch_end()\n",
    "        \n",
    "#     def on_train_end(self):\n",
    "#         for callback in self.callbacks: callback.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CallbackHandler(Callback):\n",
    "#     def __init__(self, learn, callbacks, callback_fns):\n",
    "#         self.callbacks = [cb(learn) for cb in callback_fns] + callbacks \n",
    "        \n",
    "#     def __getattr__(self, attr):\n",
    "#         if attr.startswith('on_'):\n",
    "#             for callback in sorted(self.callbacks, key=lambda x: x._order): \n",
    "#                 f = getattr(callback, attr) \n",
    "#                 if f and f(): return False # false means don't stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ModelMode:\n",
    "    TRAIN = 0\n",
    "    VALID = 1\n",
    "    TEST = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBunch:\n",
    "    def __init__(self, train_dl, valid_dl, test_dl=None, c=None):\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Runner(object): \n",
    "    def __init__(self, learn, cbs=None, cb_funcs=None):\n",
    "        self.learn = learn\n",
    "        cb_funcs = cb_funcs or []\n",
    "        cbs = cbs or []\n",
    "        self.cbs = cbs + [cb_func(self) for cb_func in cb_funcs]\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.train_counts = []\n",
    "        self.valid_losses = []\n",
    "        self.valid_counts = []\n",
    "        \n",
    "        self.batches = 0\n",
    "        \n",
    "    def train(self):\n",
    "        self.mode = ModelMode.TRAIN\n",
    "        \n",
    "    def valid(self):\n",
    "        self.mode = ModelMode.VALID\n",
    "        \n",
    "    def mode(self):\n",
    "        return self.mode \n",
    "        \n",
    "    def one_batch(self, xb, yb):\n",
    "        self.xb, self.yb = xb, yb\n",
    "        pred = self.learn.model(xb)\n",
    "        loss = self.learn.loss(pred, yb) \n",
    "         \n",
    "        if self.mode == ModelMode.TRAIN:\n",
    "            if self('on_batch_start'): return\n",
    "            self.batches += 1\n",
    "            self.train_losses.append(loss.item())\n",
    "            self.train_counts.append(len(xb))\n",
    "            loss.backward()\n",
    "            if not self('on_step_start'): self.learn.optimizer.step()\n",
    "            self.learn.optimizer.zero_grad()\n",
    "            self('on_step_end') \n",
    "            if self('on_batch_end'): return True\n",
    "            \n",
    "        if self.mode == ModelMode.VALID:\n",
    "            self.valid_losses.append(loss.item())     \n",
    "            self.valid_counts.append(len(xb))\n",
    "        \n",
    "    def all_batches(self):\n",
    "        if self('on_epoch_start'): return\n",
    "        self.epochs += 1\n",
    "        dl = self.learn.data.train_dl if self.mode == ModelMode.TRAIN else self.learn.data.valid_dl\n",
    "        for xb, yb in dl:\n",
    "            self.one_batch(xb, yb)\n",
    "        self('on_epoch_end') \n",
    "        \n",
    "    def fit(self, epochs=1):\n",
    "        self.total_batches = math.ceil(len(learn.data.train_dl) / learn.data.train_dl.bs) * epochs\n",
    "        self('on_train_start')\n",
    "        self.epochs = 0\n",
    "        for _ in range(epochs):\n",
    "            self.train()     \n",
    "            if self.all_batches(): return\n",
    "            \n",
    "            self.valid()\n",
    "            if self.all_batches(): return\n",
    "            print(\"Validation loss: {0:.4f}\".format(self.loss(ModelMode.VALID)))\n",
    "            self.epochs += 1\n",
    "        self('on_train_end')\n",
    "        \n",
    "    def loss(self, mode: int):\n",
    "        if mode == ModelMode.TRAIN:\n",
    "            values, counts = self.train_losses, self.train_counts\n",
    "        elif mode == ModelMode.VALID:\n",
    "            values, counts = self.valid_losses, self.valid_counts\n",
    "        else:\n",
    "            raise ValueError(\"mode must be one of ModelMode.TRAIN or ModelMode.VALID.\")\n",
    "            \n",
    "        total = sum(counts)\n",
    "        weights = [i/total for i in counts]\n",
    "        return sum([value*weight for value, weight in zip(values, weights)]) \n",
    "        \n",
    "    def __call__(self, name):\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, name, None)\n",
    "            if f and f(): return True\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4,  ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([3, 8, 6,  ..., 5, 6, 8]))"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(X_train, y_train)\n",
    "valid_ds = Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, 64)\n",
    "valid_dl = DataLoader(valid_ds, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "mdl = basic_model(100)\n",
    "opt = Optimizer(mdl, 1e-3)\n",
    "learn = Learn(mdl, opt, data, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_runner = Runner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_runner.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_runner.loss(ModelMode.VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BasicCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCallback:\n",
    "    _order = 0\n",
    "    \n",
    "    def __init__(self, runner: Runner):\n",
    "        self.runner = runner\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        print(self.runner.batches)\n",
    "        if self.runner.batches == 10:\n",
    "            print(\"Done!\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_runner_cb = Runner(learn, cb_funcs=[BasicCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_runner_cb.fit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler:\n",
    "    _order = 0\n",
    "    def __init__(self, start: float, stop: float, hp: str, runner: Runner):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.runner = runner \n",
    "        self.hp = hp\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        pos = self.runner.batches / self.runner.total_batches\n",
    "        new_hp = start + (self.stop - self.start) * pos\n",
    "        self.runner.learn.optimizer.hp[self.hp] = new_hp\n",
    "        if pos % 0.1 == 0:\n",
    "            print(\"The {} is now {}\".format(self.hp, new_hp)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_scheduler(start: float, stop: float, hp: str) -> partial: return partial(LinearScheduler, start, stop, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<class '__main__.LinearScheduler'>, 10, 50, 'lr')"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_scheduler(10, 50, 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LinearScheduler at 0x7ff52d6cd990>"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(basic_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineScheduler:\n",
    "    _order = 0\n",
    "    def __init__(self, start: float, stop: float, hp: str, runner: Runner):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.runner = runner \n",
    "        self.hp = hp\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        pos = self.runner.batches / self.runner.total_batches\n",
    "        new_hp = self.start + (1 + math.cos(math.pi*(1-pos))) * (self.stop - self.start) / 2\n",
    "        self.runner.learn.optimizer.hp[self.hp] = new_hp\n",
    "        if pos % 0.1 == 0 or pos == 0 or pos == 1:\n",
    "            print(\"The {} is now {} for pos {} (batch {})\".format(self.hp, new_hp, pos, self.runner.batches)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(len(data.train_dl) / data.train_dl.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_scheduler(start: float, stop: float, hp: str) -> partial: return partial(CosineScheduler, start, stop, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner_cbs = Runner(learn, cb_funcs=[cos_scheduler(1e-5, 1e-1, 'lr')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lr is now 0.0024569294678237993 for pos 0.1 (batch 782)\n",
      "Validation loss: 0.1237\n",
      "The lr is now 0.009558195366224508 for pos 0.2 (batch 1564)\n",
      "Validation loss: 0.1225\n",
      "Validation loss: 0.1217\n",
      "The lr is now 0.03455569536622451 for pos 0.4 (batch 3128)\n",
      "Validation loss: 0.1208\n",
      "Validation loss: 0.1197\n",
      "Validation loss: 0.1186\n",
      "Validation loss: 0.1173\n",
      "The lr is now 0.0904518046337755 for pos 0.8 (batch 6256)\n",
      "Validation loss: 0.1159\n",
      "Validation loss: 0.1145\n",
      "The lr is now 0.1 for pos 1.0 (batch 7820)\n",
      "Validation loss: 0.1130\n"
     ]
    }
   ],
   "source": [
    "runner_cbs.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 in range(0,4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
