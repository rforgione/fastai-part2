{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lesson2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rforgione/development/fastai-part2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo = git.Repo(search_parent_directories=True)\n",
    "repo_root = repo.git.rev_parse(\"--show-toplevel\")\n",
    "os.chdir(repo_root)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from solutions.lesson1 import *\n",
    "from fastai.datasets import *\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/development/fastai-part2/solutions/lesson1.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'\n",
    "path = download_data(MNIST_URL, ext=\".gz\")\n",
    "X_train, y_train, X_valid, y_valid = get_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([10000, 784]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000]), torch.Size([10000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        self._modules = {}\n",
    "        for idx, layer in enumerate(layers):\n",
    "            self._modules[f'layer{idx}'] = layer\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for _, layer in self.named_children(): x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = 50\n",
    "ni = X_train.shape[1]\n",
    "c = int(y_train.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([\n",
    "    nn.Linear(ni, nh),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nh, c)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer1): ReLU()\n",
       "  (layer2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 10]),\n",
       " tensor([[-0.0816,  0.0303, -0.1046,  ..., -0.1048, -0.0573,  0.0998],\n",
       "         [ 0.0100, -0.0479, -0.0986,  ..., -0.1628, -0.0820,  0.1606],\n",
       "         [-0.0561,  0.0262, -0.0808,  ..., -0.1641, -0.0790,  0.0569],\n",
       "         ...,\n",
       "         [-0.1743,  0.0557, -0.1266,  ..., -0.0885, -0.0888,  0.1244],\n",
       "         [-0.1408, -0.0087, -0.1271,  ..., -0.0401, -0.1106,  0.1369],\n",
       "         [-0.1068,  0.0717, -0.1291,  ..., -0.0958, -0.0599,  0.0835]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the naive way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return (vec.exp() / vec.exp().sum(-1)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert log_softmax(torch.tensor([[1.,2.,3.]])).exp().sum() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the numerically friendly way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return vec - vec.exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert log_softmax(torch.tensor([[1.,2.,3.]])).exp().sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(output, target):\n",
    "    return -output[range(len(output)), target].mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_output = torch.tensor([\n",
    "    [0.5, 0.5],\n",
    "    [0.3, 0.7]\n",
    "])\n",
    "\n",
    "fake_target = torch.tensor([\n",
    "    0,\n",
    "    1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6931471805599453"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_first = np.log(0.5); log_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.35667494393873245"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_second = np.log(0.7); log_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_nll = -(log_first + log_second)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5249)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll(torch.log(fake_output), fake_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(nll(fake_output.log(), fake_target), torch.tensor(correct_nll).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogSumExp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogSumExp is a way of making the sum of exponents more numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(vec):\n",
    "    max_elem = vec.max()\n",
    "    return max_elem + (vec - max_elem).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.tensor([1,2,3]).float().exp().sum().log() == logsumexp(torch.tensor([1,2,3]).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a way of making the log of a sum of exponentials more numerically stable. You can take advantage of the fact that dividing two exponentials results in subtracting their exponents. You can factor out the largest exponential, which subtracts its exponent from each of the other exponents. Then you can take advantage of the log rules that state that log(ab) = log(a) + log(b), which allows you to add the highest exponential to the log of the sum of the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logsumexp(torch.tensor([1,2,3]).float()) == torch.logsumexp(torch.tensor([1,2,3]).float(), dim=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_softmax in terms of LogSumExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(vec):\n",
    "    return vec - vec.logsumexp(dim=(0,), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(torch.tensor([1,2,3]).float()) == torch.log_softmax(torch.tensor([1,2,3]).float(), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 5],\n",
       "       [5, 6, 7]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2,3], [3,4,5], [5,6,7]])[:, [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_example = torch.tensor([\n",
    "    [0,0,1],\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_example[range(len(grad_example)), grad_example.argmax(dim=1)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 2],\n",
       "        [2, 0, 0],\n",
       "        [0, 2, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [1, 0],\n",
       "        [2, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 2],\n",
       "         [0, 2, 0]],\n",
       "\n",
       "        [[2, 0, 0],\n",
       "         [0, 0, 2]],\n",
       "\n",
       "        [[0, 2, 0],\n",
       "         [2, 0, 0]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_example[grad_example.nonzero()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0,0,0,1,2],\n",
    "    [3,5,3,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 5, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero = t.nonzero()\n",
    "xs = nonzero[:,0]\n",
    "ys = nonzero[:,1]\n",
    "t[xs, ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = torch.zeros_like(t)\n",
    "tz[xs, ys] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(nn.Module):\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = torch.log_softmax(self.input, dim=-1)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self): \n",
    "        correct_labels = (self.output.grad > 0).float()\n",
    "        msg = \"LogSoftmax must be used with CE loss, found more than one output dim with nonzero gradients for the same example\"\n",
    "        assert correct_labels.sum(axis=0).max() == 1, msg\n",
    "        self.input.grad = (correct_labels - self.output) * self.output.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([50000, 10]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = Model([\n",
    "    nn.Linear(784,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, c),\n",
    "    LogSoftmax()\n",
    "])\n",
    "X_train.shape, mdl(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2 = Model([\n",
    "    nn.Linear(784,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, c),\n",
    "    nn.LogSoftmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1018, 0.1082, 0.1049,  ..., 0.1113, 0.0977, 0.0842],\n",
       "        [0.0997, 0.1145, 0.0936,  ..., 0.1044, 0.0988, 0.0877],\n",
       "        [0.1007, 0.0976, 0.0882,  ..., 0.0924, 0.1099, 0.1043],\n",
       "        ...,\n",
       "        [0.1068, 0.1060, 0.0790,  ..., 0.1042, 0.1028, 0.0883],\n",
       "        [0.1077, 0.0857, 0.0884,  ..., 0.1022, 0.1068, 0.1010],\n",
       "        [0.1066, 0.0919, 0.0892,  ..., 0.1022, 0.1001, 0.0900]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(mdl(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = loss(mdl(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3]]).float()\n",
    "l = LogSoftmax()\n",
    "l.forward(a)\n",
    "l.output.grad = torch.tensor([[0,0,10]]).float()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.4076, -1.4076, -0.4076]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log_softmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000, 14.0761]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(log_softmax(torch.tensor([1,2,3]).float()), torch.log_softmax(torch.tensor([1,2,3]).float(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer0', Linear(in_features=784, out_features=100, bias=True)), ('layer1', ReLU()), ('layer2', Linear(in_features=100, out_features=10, bias=True)), ('layer3', LogSoftmax())]\n"
     ]
    }
   ],
   "source": [
    "print(list(mdl2.named_children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, bs=64, lr=1e-3, epochs=1):\n",
    "    n_batches = X_train.shape[0] // bs + 1\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * bs\n",
    "            end_idx = (i + 1) * bs\n",
    "            X_b, y_b = X_train[start_idx:end_idx], y_train[start_idx:end_idx] \n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for _, layer in model.named_children():\n",
    "                    if hasattr(layer, 'weight'):\n",
    "                        layer.weight.sub_(layer.weight.grad * lr)\n",
    "                        layer.bias.sub_(layer.bias.grad * lr)\n",
    "                        layer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (layer1): ReLU()\n",
       "  (layer2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (layer3): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has it\n",
      "has it\n"
     ]
    }
   ],
   "source": [
    "for i,l in mdl.named_children(): \n",
    "    if hasattr(l, 'weight'): \n",
    "        print('has it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rforgione/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1718132495880127\n"
     ]
    }
   ],
   "source": [
    "fit(mdl2, 64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41941094398498535\n"
     ]
    }
   ],
   "source": [
    "fit(mdl, 64, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nograd(f):\n",
    "    @wraps(f)\n",
    "    def inner(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return f(*args, **kwargs)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, lr):\n",
    "        self.lr = lr\n",
    "        params = []\n",
    "        for _, layer in model.named_children():\n",
    "            if hasattr(layer, 'weight'):\n",
    "                params.append(layer)\n",
    "        self.params = params\n",
    "    \n",
    "    @nograd\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.weight.sub_(param.weight.grad * self.lr)\n",
    "            param.bias.sub_(param.bias.grad * self.lr)\n",
    "                \n",
    "    @nograd\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.zero_grad() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, bs=64, epochs=1):\n",
    "    n_batches = X_train.shape[0] // bs\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * bs\n",
    "            end_idx = (i + 1) * bs\n",
    "            X_b, y_b = X_train[start_idx:end_idx], y_train[start_idx:end_idx] \n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model(nh, torch_softmax=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(784, nh),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(nh, 10),\n",
    "        nn.LogSoftmax() if torch_softmax else LogSoftmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7323116660118103\n"
     ]
    }
   ],
   "source": [
    "mdl = basic_model(100)\n",
    "opt = Optimizer(mdl, 1e-3)\n",
    "fit(mdl, opt, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rforgione/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7221540212631226\n"
     ]
    }
   ],
   "source": [
    "mdl2 = basic_model(100, True)\n",
    "opt2 = Optimizer(mdl2, 1e-3)\n",
    "fit(mdl2, opt2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([4, 1, 9]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [(1,2), (3,4), (5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 5), (2, 4, 6)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialSampler:\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.ds)):\n",
    "            yield(i)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, ds, bs, sampler=None):\n",
    "        self.ds = ds\n",
    "        self.bs = bs\n",
    "        \n",
    "        if callable(sampler):\n",
    "            self.sampler = sampler(ds)\n",
    "        else:\n",
    "            self.sampler = sampler or SequentialSampler(ds)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        it = iter(self.sampler)\n",
    "        nbatches = len(self.sampler)//self.bs + 1\n",
    "        for _ in range(nbatches):\n",
    "            try:\n",
    "                idxs = []\n",
    "                for _ in range(self.bs):\n",
    "                    idxs.append(next(it))\n",
    "                yield self.ds[idxs]\n",
    "            except StopIteration:\n",
    "                yield self.ds[idxs] # flush the final partial batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(d, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([len(yb) for xb,yb in dl]), max([len(yb) for xb,yb in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, dl, epochs=1):\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        for X_b, y_b in dl:\n",
    "            pred = model(X_b)\n",
    "            loss = nll(pred, y_b)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = basic_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(mod, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42209887504577637\n"
     ]
    }
   ],
   "source": [
    "fit(mod, opt, dl, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    _order = 0\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_batch_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_forward_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_forward_end(self): \n",
    "        pass\n",
    "    \n",
    "    def on_backward_start(self): \n",
    "        pass\n",
    "    \n",
    "    def on_backward_end(self): \n",
    "        pass\n",
    "        \n",
    "    def on_batch_end(self):\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learn:\n",
    "    def __init__(self, model, optimizer, data, loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.data = data\n",
    "        self.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CallbackHandler:\n",
    "#     def __init__(self, learn, callbacks, callback_fns):\n",
    "#         self.callbacks = [cb(learn) for cb in callback_fns] + callbacks \n",
    "        \n",
    "#     def on_train_start(self):\n",
    "#         for callback in self.callbacks: callback.on_train_start()\n",
    "        \n",
    "#     def on_epoch_start(self):\n",
    "#         for callback in self.callbacks: callback.on_epoch_start()\n",
    "        \n",
    "#     def on_batch_start(self): \n",
    "#         for callback in self.callbacks: callback.on_batch_start()\n",
    "            \n",
    "#     def on_forward_start(self): \n",
    "#         for callback in self.callbacks: callback.on_forward_start()\n",
    "            \n",
    "#     def on_forward_end(self): \n",
    "#         for callback in self.callbacks: callback.on_forward_end()\n",
    "            \n",
    "#     def on_backward_start(self): \n",
    "#         for callback in self.callbacks: callback.on_backward_start()\n",
    "            \n",
    "#     def on_backward_end(self): \n",
    "#         for callback in self.callbacks: callback.on_backward_end()\n",
    "        \n",
    "#     def on_batch_end(self):\n",
    "#         for callback in self.callbacks: callback.on_batch_end()\n",
    "        \n",
    "#     def on_epoch_end(self):\n",
    "#         for callback in self.callbacks: callback.on_epoch_end()\n",
    "        \n",
    "#     def on_train_end(self):\n",
    "#         for callback in self.callbacks: callback.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CallbackHandler(Callback):\n",
    "#     def __init__(self, learn, callbacks, callback_fns):\n",
    "#         self.callbacks = [cb(learn) for cb in callback_fns] + callbacks \n",
    "        \n",
    "#     def __getattr__(self, attr):\n",
    "#         if attr.startswith('on_'):\n",
    "#             for callback in sorted(self.callbacks, key=lambda x: x._order): \n",
    "#                 f = getattr(callback, attr) \n",
    "#                 if f and f(): return False # false means don't stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMode:\n",
    "    TRAIN = 0\n",
    "    VALID = 1\n",
    "    TEST = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch:\n",
    "    def __init__(self, train_dl, valid_dl, test_dl=None, c=None):\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(object): \n",
    "    def __init__(self, learn, cbs=None, cb_funcs=None):\n",
    "        self.learn = learn\n",
    "        cb_funcs = cb_funcs or []\n",
    "        cbs = cbs or []\n",
    "        self.cbs = cb + [cb_func(learn) for cb_func in cb_funcs]\n",
    "        \n",
    "    def train(self):\n",
    "        self.mode = ModelMode.TRAIN\n",
    "        \n",
    "    def valid(self):\n",
    "        self.mode = ModelMode.VALID\n",
    "        \n",
    "    @property\n",
    "    def mode(self):\n",
    "        return self.mode \n",
    "        \n",
    "    def one_batch(self, xb, yb):\n",
    "        self.xb, self.yb = xb, yb\n",
    "        if self('on_batch_start'): return\n",
    "        pred = self.learn.model(xb)\n",
    "        self.loss = self.learn.loss(pred, yb)\n",
    "        \n",
    "        if self.mode() == ModelMode.TRAIN:\n",
    "            if not self('on_step_start'): optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            self('on_step_end') \n",
    "        \n",
    "    def all_batches(self):\n",
    "        if self('on_epoch_start'): return\n",
    "        self.epochs += 1\n",
    "        dl = self.learn.data.train_dl if self.mode == ModelMode.TRAIN else self.learn.data.valid_dl\n",
    "        for xb, yb in dl:\n",
    "            self.xb, self.yb = xb, yb\n",
    "            if not self('on_batch_start'): self.one_batch(xb, yb)\n",
    "            self('on_batch_end')\n",
    "        self('on_epoch_end') \n",
    "        \n",
    "    def fit(learn, cb_handler=None, epochs=1):\n",
    "        self('on_train_start')\n",
    "        self.epochs = 0\n",
    "        for _ in range(epochs):\n",
    "            self.train()     \n",
    "            self.all_batches()\n",
    "            \n",
    "            self.valid()\n",
    "            self.all_batches()\n",
    "        self('on_train_end')\n",
    "        \n",
    "    def __call__(self, name):\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, name, None)\n",
    "            if f and f(): return True\n",
    "        return False "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
